name: dbt CI/CD Pipeline

on:
  push:
    branches:
      - main
      - develop
  pull_request:
    branches:
      - main
      - develop
  schedule:
    # Run daily at 6 AM UTC (adjust timezone as needed)
    - cron: '0 6 * * *'
  workflow_dispatch:
    # Manual trigger with input parameters
    inputs:
      country:
        description: 'Country to extract data for'
        required: false
        default: 'Singapore'
        type: choice
        options:
          - Singapore
          - Monaco
          - Italy
          - United States
          - All
      year:
        description: 'Year to extract data for'
        required: false
        default: '2024'
        type: string

env:
  DBT_PROFILES_DIR: ./
  PYTHON_VERSION: '3.11'
  # Default extraction parameters (can be overridden)
  DEFAULT_COUNTRY: 'Singapore'
  DEFAULT_YEAR: '2024'

jobs:
  # Job 1: Data Extraction
  extract-data:
    name: Extract F1 Data from OpenF1 API
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Determine extraction parameters
        id: params
        run: |
          # Use workflow inputs if available (manual trigger)
          # Otherwise use GitHub variables or defaults
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            COUNTRY="${{ github.event.inputs.country }}"
            YEAR="${{ github.event.inputs.year }}"
          else
            COUNTRY="${{ vars.F1_EXTRACTION_COUNTRY || env.DEFAULT_COUNTRY }}"
            YEAR="${{ vars.F1_EXTRACTION_YEAR || env.DEFAULT_YEAR }}"
          fi
          
          echo "country=$COUNTRY" >> $GITHUB_OUTPUT
          echo "year=$YEAR" >> $GITHUB_OUTPUT
          echo "Extracting data for: $COUNTRY, Year: $YEAR"

      - name: Run F1 data extraction script
        run: |
          python run_data_extraction.py \
            --country "${{ steps.params.outputs.country }}" \
            --year "${{ steps.params.outputs.year }}"
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          BQ_DATASET: ${{ secrets.BQ_DATASET }}

      - name: Verify data extraction
        run: |
          bq query --use_legacy_sql=false \
            "SELECT 
               'drivers' as table_name, COUNT(*) as row_count 
             FROM \`${{ secrets.GCP_PROJECT_ID }}.${{ secrets.BQ_DATASET }}.drivers\`
             UNION ALL
             SELECT 'laps', COUNT(*) 
             FROM \`${{ secrets.GCP_PROJECT_ID }}.${{ secrets.BQ_DATASET }}.laps\`
             UNION ALL
             SELECT 'locations', COUNT(*) 
             FROM \`${{ secrets.GCP_PROJECT_ID }}.${{ secrets.BQ_DATASET }}.locations\`
             UNION ALL
             SELECT 'pit', COUNT(*) 
             FROM \`${{ secrets.GCP_PROJECT_ID }}.${{ secrets.BQ_DATASET }}.pit\`"

  # Job 2: dbt Build and Test
  dbt-run:
    name: dbt Build and Test
    runs-on: ubuntu-latest
    needs: extract-data  # Wait for extraction to complete
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dbt and dependencies
        run: |
          pip install --upgrade pip
          pip install dbt-bigquery==1.8.0
          pip install -r requirements.txt

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up dbt profile
        run: |
          mkdir -p ~/.dbt
          echo "${{ secrets.DBT_PROFILES_YML }}" > ~/.dbt/profiles.yml

      - name: Install dbt dependencies
        run: |
          cd dbt_project
          dbt deps

      - name: Check dbt connection
        run: |
          cd dbt_project
          dbt debug

      - name: Run dbt source freshness
        run: |
          cd dbt_project
          dbt source freshness
        continue-on-error: true

      - name: Run dbt staging models
        run: |
          cd dbt_project
          dbt run --select staging.*

      - name: Run dbt dimension models
        run: |
          cd dbt_project
          dbt run --select dim_*

      - name: Run dbt fact models
        run: |
          cd dbt_project
          dbt run --select fct_*

      - name: Run dbt tests
        run: |
          cd dbt_project
          dbt test --store-failures

      - name: Generate dbt docs
        run: |
          cd dbt_project
          dbt docs generate

      - name: Upload dbt artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: dbt-artifacts
          path: |
            dbt_project/target/manifest.json
            dbt_project/target/run_results.json
            dbt_project/target/catalog.json
          retention-days: 30

  # Job 3: Data Quality Checks
  data-quality:
    name: Data Quality Validation
    runs-on: ubuntu-latest
    needs: dbt-run
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Run custom data quality checks
        run: |
          # Check for data freshness
          bq query --use_legacy_sql=false \
            "SELECT 
               MAX(extracted_at) as latest_extraction,
               TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), MAX(extracted_at), HOUR) as hours_since_last_update
             FROM \`${{ secrets.GCP_PROJECT_ID }}.${{ secrets.BQ_DATASET }}.drivers\`"
          
          # Check row counts are reasonable
          bq query --use_legacy_sql=false \
            "SELECT 
               COUNT(*) as driver_count
             FROM \`${{ secrets.GCP_PROJECT_ID }}.mart.dim_drivers\`
             HAVING COUNT(*) < 10 OR COUNT(*) > 30"

  # Job 4: Notification
  notify:
    name: Send Notification
    runs-on: ubuntu-latest
    needs: [extract-data, dbt-run, data-quality]
    if: always()
    
    steps:
      - name: Send Slack notification on success
        if: ${{ needs.dbt-run.result == 'success' }}
        uses: slackapi/slack-github-action@v1.25.0
        with:
          payload: |
            {
              "text": "✅ F1 Data Pipeline Success",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*F1 Data Pipeline Completed Successfully*\n• Data Extraction: ✅\n• dbt Run: ✅\n• Tests: ✅\n• Commit: ${{ github.sha }}\n• Branch: ${{ github.ref_name }}"
                  }
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

      - name: Send Slack notification on failure
        if: ${{ needs.dbt-run.result == 'failure' || needs.extract-data.result == 'failure' }}
        uses: slackapi/slack-github-action@v1.25.0
        with:
          payload: |
            {
              "text": "❌ F1 Data Pipeline Failed",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*F1 Data Pipeline Failed*\n• Check the workflow logs\n• Commit: ${{ github.sha }}\n• Branch: ${{ github.ref_name }}\n• <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Run>"
                  }
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}