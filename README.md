# F1 Analytics Pipeline

A production-ready ELT pipeline that extracts Formula 1 telemetry data from the OpenF1 API and transforms it into analytics-ready dimensional models in BigQuery.

**Tech Stack:** Python Â· BigQuery Â· dbt Â· GitHub Actions

---

## Features

âœ… **Idempotent data ingestion** - Safe to re-run, no duplicates  
âœ… **Dimensional modeling** - Star schema with fact and dimension tables  
âœ… **Automated CI/CD** - GitHub Actions for extraction, transformation, and testing  
âœ… **Comprehensive testing** - 28+ data quality tests  
âœ… **Production-ready** - Error handling, logging, monitoring  

---

## Quick Start

### Prerequisites

- Python 3.9+
- Google Cloud Platform account with BigQuery enabled
- dbt installed (`pip install dbt-bigquery`)

### Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd f1-analytics-pipeline

# Install dependencies
pip install -r requirements.txt

# Configure GCP credentials
export GOOGLE_APPLICATION_CREDENTIALS="path/to/service-account-key.json"
```

### Run the Pipeline

**1. Extract F1 Data**
```bash
# Extract by race
python run_data_extraction.py --country "Singapore" --year 2024

# Or extract by session ID
python run_data_extraction.py --session_key 9472
```

**2. Transform with dbt**
```bash
cd dbt_project

# Run all models
dbt run

# Run tests
dbt test

# Generate documentation
dbt docs generate
dbt docs serve
```

---

## Project Structure

```
â”œâ”€â”€ run_data_extraction.py       # Main extraction script
â”œâ”€â”€ config.py                    # Configuration settings
â”œâ”€â”€ logger.py                    # Custom logging
â”œâ”€â”€ requirements.txt             # Python dependencies
â”‚
â”œâ”€â”€ data_ingestion/
â”‚   â””â”€â”€ data_extractor.py        # F1BigQueryExtractor class
â”‚
â”œâ”€â”€ dbt_project/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/             # Staging views (raw data cleanup)
â”‚   â”‚   â”œâ”€â”€ intermediate/        # Dimension tables
â”‚   â”‚   â””â”€â”€ marts/              # Fact tables (analytics-ready)
â”‚   â”œâ”€â”€ tests/                   # Custom dbt tests
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ dbt-ci.yml           # Production pipeline
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ architecture.md          # Architecture documentation
â”‚
â””â”€â”€ logs/                        # Extraction logs
```

---

## Data Model

**Star Schema Design:**

**Dimensions:**
- `dim_drivers` - Driver profiles (name, team, country)
- `dim_sessions` - Race sessions (circuit, date, type)

**Facts:**
- `fct_lap_summary` - Aggregated lap statistics

---

## CI/CD Pipeline

The pipeline runs automatically via GitHub Actions:

1. **Extract** data from OpenF1 API
2. **Load** into BigQuery raw tables (idempotent MERGE)
3. **Transform** using dbt (staging â†’ dimensions â†’ facts)
4. **Test** data quality (28+ tests)

**Triggers:**
- Push to `main` branch
- Pull requests
- Daily schedule (6 AM UTC)
- Manual workflow dispatch

---

## Configuration

### BigQuery Setup

1. Create two datasets in your GCP project:
   ```sql
   CREATE SCHEMA `your-project.f1_raw_data`;
   CREATE SCHEMA `your-project.f1_analytics`;
   ```

2. Create a service account with these roles:
   - `roles/bigquery.dataEditor`
   - `roles/bigquery.jobUser`

3. Update `config.py`:
   ```python
   PROJECT_ID = "your-project-id"
   DATASET_ID = "f1_raw_data"
   ```

### dbt Configuration

Update `dbt_project/profiles.yml`:
```yaml
f1_project:
  target: dev
  outputs:
    dev:
      type: bigquery
      method: service-account
      project: your-project-id
      dataset: f1_analytics
      keyfile: /path/to/service-account-key.json
      location: US
```

---

## Testing

**Run all tests:**
```bash
cd dbt_project
dbt test
```

**Test coverage:**
- Source freshness checks (4 sources)
- Schema validation (not_null, unique)
- Referential integrity (foreign keys)
- Business logic (lap times, speeds)

---

## Documentation

ðŸ“– **[Architecture Documentation](docs/architecture.md)** - Detailed system design, scaling strategies, and production considerations

ðŸ“Š **dbt Documentation** - Auto-generated lineage and model docs:
```bash
dbt docs generate
dbt docs serve
```

---

---

## Troubleshooting

**Issue: API returns 422 error**
- Solution: Add more specific filters (session_key, date range)
- The API rejects requests for too much data at once

**Issue: dbt test failures**
- Check logs: `cd dbt_project && dbt test --debug`
- Review failed test in `target/compiled/` directory

**Issue: BigQuery permission denied**
- Verify service account has `bigquery.dataEditor` role
- Check `GOOGLE_APPLICATION_CREDENTIALS` is set correctly

---
